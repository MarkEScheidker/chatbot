{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223dcf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[44 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/requirements.py\", line 35, in __init__\n",
      "  \u001b[31m   \u001b[0m     parsed = _parse_requirement(requirement_string)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/_parser.py\", line 64, in parse_requirement\n",
      "  \u001b[31m   \u001b[0m     return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/_parser.py\", line 82, in _parse_requirement\n",
      "  \u001b[31m   \u001b[0m     url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/_parser.py\", line 126, in _parse_requirement_details\n",
      "  \u001b[31m   \u001b[0m     marker = _parse_requirement_marker(\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/_parser.py\", line 147, in _parse_requirement_marker\n",
      "  \u001b[31m   \u001b[0m     tokenizer.raise_syntax_error(\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "  \u001b[31m   \u001b[0m     raise ParserSyntaxError(\n",
      "  \u001b[31m   \u001b[0m setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-kix7o2k0/tensorflow-gpu_8a38b8be30cc48c2b865b7c56afc494c/setup.py\", line 40, in <module>\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py\", line 102, in setup\n",
      "  \u001b[31m   \u001b[0m     _install_setup_requires(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py\", line 73, in _install_setup_requires\n",
      "  \u001b[31m   \u001b[0m     dist.parse_config_files(ignore_option_errors=True)\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 629, in parse_config_files\n",
      "  \u001b[31m   \u001b[0m     self._finalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 364, in _finalize_requires\n",
      "  \u001b[31m   \u001b[0m     self._normalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 379, in _normalize_requires\n",
      "  \u001b[31m   \u001b[0m     self.install_requires = list(map(str, _reqs.parse(install_requires)))\n",
      "  \u001b[31m   \u001b[0m                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/setuptools/_vendor/packaging/requirements.py\", line 37, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise InvalidRequirement(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hRequirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.0.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.42.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 20:43:11.117839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow-gpu\n",
    "!pip install tf-keras\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2f48c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-05-03 20:43:13.710896: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.739773: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.739807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.741290: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.741324: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.741340: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.835430: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.835466: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.835473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-03 20:43:13.835498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:13.835517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "eos_token = tokenizer.sep_token_id\n",
    "pad_token = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903e3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "target_tokens = []\n",
    "max_length = 64\n",
    "\n",
    "with open(\"dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        question, answer = line.strip().split('\\t')\n",
    "\n",
    "        # Encode the texts. Adding the EOS token is important for GPT to know when a sentence ends\n",
    "        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "        answer_tokens = tokenizer.encode(answer, add_special_tokens=False) + [eos_token] # Ensure EOS token is added to the answer\n",
    "\n",
    "        # Construct sequences of max_length or shorter\n",
    "        for i in range(0, len(answer_tokens)):\n",
    "            # Input sequence: question + end token + part of the answer\n",
    "            seq = question_tokens + [eos_token] + answer_tokens[:i]\n",
    "            if len(seq) > max_length:\n",
    "                break\n",
    "\n",
    "            # Ensure sequences are padded to have uniform length\n",
    "            input_sequences.append(seq + [pad_token] * (max_length - len(seq)))  # Padding\n",
    "\n",
    "            # Target token is the next token in the answer\n",
    "            target_tokens.append(answer_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21d3a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, how are you doing? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how about [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how about yourself [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how about yourself? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "i\n",
      "'\n",
      "m\n",
      "f i n e\n",
      ".\n",
      "h o w\n",
      "a b o u t\n",
      "y o u r s e l f\n",
      "?\n",
      "[ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for seq in input_sequences[:10]:\n",
    "    #print(seq)\n",
    "    print(tokenizer.decode(np.array(seq), clean_up_tokenization_spaces=True))\n",
    "    print('\\n')\n",
    "          \n",
    "for seq in target_tokens[:10]:\n",
    "    print(tokenizer.decode(np.array(seq), clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = tf.constant(input_sequences, dtype=tf.int32)\n",
    "target_tokens = tf.constant(target_tokens, dtype=tf.int32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_tokens))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fefc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0cde2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids (InputLayer)      [(None, 64)]              0         \n",
      "                                                                 \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPo   109482240 \n",
      " )                           olingAndCrossAttentions             \n",
      "                             (last_hidden_state=(Non             \n",
      "                             e, 64, 768),                        \n",
      "                              pooler_output=(None, 7             \n",
      "                             68),                                \n",
      "                              past_key_values=None,              \n",
      "                             hidden_states=None, att             \n",
      "                             entions=None, cross_att             \n",
      "                             entions=None)                       \n",
      "                                                                 \n",
      " tf.__operators__.getitem (  (None, 768)               0         \n",
      " SlicingOpLambda)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30522)             23471418  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132953658 (507.18 MB)\n",
      "Trainable params: 132953658 (507.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "outputs = bert_model(input_ids)[0]  # We use only the last hidden state\n",
    "cls_output = outputs[:, -1, :]  # We use the output of the last token for prediction\n",
    "predictions = tf.keras.layers.Dense(vocab_size, activation='softmax')(cls_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427583c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 20:43:28.437812: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.437904: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.437924: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438148: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-03 20:43:28.438181: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-05-03 20:43:28.438261: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438280: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438502: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-03 20:43:28.438528: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-03 20:43:28.438538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print(f\"Default GPU Device: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ade5216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714769045.391304    3845 service.cc:145] XLA service 0x7fb54020b130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1714769045.391370    3845 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2024-05-03 20:44:05.400231: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-03 20:44:05.420987: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
      "I0000 00:00:1714769045.469351    3845 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139/1139 [==============================] - 222s 156ms/step - loss: 5.7263\n",
      "Epoch 2/25\n",
      "1139/1139 [==============================] - 171s 150ms/step - loss: 5.2231\n",
      "Epoch 3/25\n",
      "1139/1139 [==============================] - 165s 145ms/step - loss: 5.1820\n",
      "Epoch 4/25\n",
      "1139/1139 [==============================] - 165s 145ms/step - loss: 5.1605\n",
      "Epoch 5/25\n",
      "1139/1139 [==============================] - 168s 148ms/step - loss: 5.1480\n",
      "Epoch 6/25\n",
      "1139/1139 [==============================] - 167s 147ms/step - loss: 5.1382\n",
      "Epoch 7/25\n",
      "1139/1139 [==============================] - 173s 152ms/step - loss: 5.1341\n",
      "Epoch 8/25\n",
      "1139/1139 [==============================] - 173s 152ms/step - loss: 5.1239\n",
      "Epoch 9/25\n",
      "1139/1139 [==============================] - 165s 145ms/step - loss: 5.1142\n",
      "Epoch 10/25\n",
      "1139/1139 [==============================] - 172s 151ms/step - loss: 5.1127\n",
      "Epoch 11/25\n",
      "1139/1139 [==============================] - 170s 150ms/step - loss: 5.1104\n",
      "Epoch 12/25\n",
      "1139/1139 [==============================] - 171s 150ms/step - loss: 5.1114\n",
      "Epoch 13/25\n",
      "1139/1139 [==============================] - 171s 150ms/step - loss: 5.1081\n",
      "Epoch 14/25\n",
      "1139/1139 [==============================] - 171s 150ms/step - loss: 5.1033\n",
      "Epoch 15/25\n",
      "1139/1139 [==============================] - 171s 150ms/step - loss: 5.0983\n",
      "Epoch 16/25\n",
      "1139/1139 [==============================] - 170s 150ms/step - loss: 5.0973\n",
      "Epoch 17/25\n",
      "1139/1139 [==============================] - 175s 154ms/step - loss: 5.1113\n",
      "Epoch 18/25\n",
      "1139/1139 [==============================] - 174s 153ms/step - loss: 5.1208\n",
      "Epoch 19/25\n",
      "1139/1139 [==============================] - 170s 149ms/step - loss: 5.1243\n",
      "Epoch 20/25\n",
      "1139/1139 [==============================] - 170s 149ms/step - loss: 5.1256\n",
      "Epoch 21/25\n",
      "1139/1139 [==============================] - 172s 151ms/step - loss: 5.1116\n",
      "Epoch 22/25\n",
      "1139/1139 [==============================] - 167s 146ms/step - loss: 5.1194\n",
      "Epoch 23/25\n",
      "1139/1139 [==============================] - 171s 150ms/step - loss: 5.1142\n",
      "Epoch 24/25\n",
      "1139/1139 [==============================] - 176s 155ms/step - loss: 5.1178\n",
      "Epoch 25/25\n",
      "1139/1139 [==============================] - 167s 147ms/step - loss: 5.1175\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c214fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_transformer_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_transformer_model/assets\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"my_transformer_model\"\n",
    "tf.saved_model.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9538263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "eos_token = tokenizer.sep_token_id\n",
    "\n",
    "model_save_path = \"my_transformer_model\"\n",
    "loaded_model = tf.saved_model.load(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aa973c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_until_sep_token(starting_text, max_tokens=50):\n",
    "    text = starting_text\n",
    "    for _ in range(max_tokens):\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False) + [eos_token]\n",
    "        \n",
    "        # Pad token_ids to ensure the input length is exactly 64\n",
    "        if len(token_ids) < 64:\n",
    "            token_ids += [pad_token] * (64 - len(token_ids))\n",
    "        elif len(token_ids) > 64:\n",
    "            token_ids = token_ids[-64:]  # Truncate to the last 64 tokens if exceeding the length\n",
    "        \n",
    "        input_sequence = tf.constant([token_ids], dtype=tf.int32)\n",
    "\n",
    "        # Using the serving function with named arguments as expected by the model\n",
    "        serving_fn = loaded_model.signatures['serving_default']\n",
    "        predictions = serving_fn(input_ids=input_sequence)\n",
    "\n",
    "        # Accessing the prediction using the 'dense' key\n",
    "        predicted_token_id = tf.argmax(predictions['dense'], axis=-1).numpy()[0]\n",
    "\n",
    "        #if predicted_token_id == eos_token:\n",
    "            #break\n",
    "        \n",
    "        # Decode the predicted token ID to the corresponding token\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        text += predicted_token\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efd6f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: hello, how are you?[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"hello, how are you?\"\n",
    "generated_text = generate_until_sep_token(starting_text, max_tokens=50)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753992e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
