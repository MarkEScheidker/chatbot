{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223dcf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\marks\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marks\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marks\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marks\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [44 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 35, in __init__\n",
      "      parsed = _parse_requirement(requirement_string)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 64, in parse_requirement\n",
      "      return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 82, in _parse_requirement\n",
      "      url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 126, in _parse_requirement_details\n",
      "      marker = _parse_requirement_marker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 147, in _parse_requirement_marker\n",
      "      tokenizer.raise_syntax_error(\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_tokenizer.py\", line 165, in raise_syntax_error\n",
      "      raise ParserSyntaxError(\n",
      "  setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  \n",
      "  The above exception was the direct cause of the following exception:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\marks\\AppData\\Local\\Temp\\pip-install-xx23zapd\\tensorflow-gpu_d4d95df7448241ed948afaeba4bdc264\\setup.py\", line 40, in <module>\n",
      "      setuptools.setup()\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\__init__.py\", line 106, in setup\n",
      "      _install_setup_requires(attrs)\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\__init__.py\", line 77, in _install_setup_requires\n",
      "      dist.parse_config_files(ignore_option_errors=True)\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 900, in parse_config_files\n",
      "      self._finalize_requires()\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 597, in _finalize_requires\n",
      "      self._move_install_requirements_markers()\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 637, in _move_install_requirements_markers\n",
      "      inst_reqs = list(_reqs.parse(spec_inst_reqs))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 37, in __init__\n",
      "      raise InvalidRequirement(str(e)) from e\n",
      "  setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "WARNING:tensorflow:From C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow-gpu\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2f48c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "eos_token = tokenizer.sep_token_id\n",
    "pad_token = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903e3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "target_tokens = []\n",
    "max_length = 64\n",
    "\n",
    "with open(\"dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        question, answer = line.strip().split('\\t')\n",
    "\n",
    "        # Encode the texts. Adding the EOS token is important for GPT to know when a sentence ends\n",
    "        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "        answer_tokens = tokenizer.encode(answer, add_special_tokens=False) + [eos_token] # Ensure EOS token is added to the answer\n",
    "\n",
    "        # Construct sequences of max_length or shorter\n",
    "        for i in range(0, len(answer_tokens)):\n",
    "            # Input sequence: question + end token + part of the answer\n",
    "            seq = question_tokens + [eos_token] + answer_tokens[:i]\n",
    "            if len(seq) > max_length:\n",
    "                break\n",
    "\n",
    "            # Ensure sequences are padded to have uniform length\n",
    "            input_sequences.append(seq + [pad_token] * (max_length - len(seq)))  # Padding\n",
    "\n",
    "            # Target token is the next token in the answer\n",
    "            target_tokens.append(answer_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21d3a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, how are you doing? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how about [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how about yourself [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "hi, how are you doing? [SEP] i'm fine. how about yourself? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "i\n",
      "'\n",
      "m\n",
      "f i n e\n",
      ".\n",
      "h o w\n",
      "a b o u t\n",
      "y o u r s e l f\n",
      "?\n",
      "[ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for seq in input_sequences[:10]:\n",
    "    #print(seq)\n",
    "    print(tokenizer.decode(np.array(seq), clean_up_tokenization_spaces=True))\n",
    "    print('\\n')\n",
    "          \n",
    "for seq in target_tokens[:10]:\n",
    "    print(tokenizer.decode(np.array(seq), clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to TensorFlow tensors\n",
    "input_sequences = tf.constant(input_sequences, dtype=tf.int16)\n",
    "target_tokens = tf.constant(target_tokens, dtype=tf.int16)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_tokens))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fefc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0cde2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 256  # Size of the embedding vectors\n",
    "rnn_units = 512  # Number of units in the LSTM layer\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(rnn_units),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427583c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print(f\"Default GPU Device: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ade5216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1139/1139 [==============================] - 357s 311ms/step - loss: 5.1314\n",
      "Epoch 2/3\n",
      "1139/1139 [==============================] - 364s 320ms/step - loss: 3.9343\n",
      "Epoch 3/3\n",
      "1139/1139 [==============================] - 366s 321ms/step - loss: 3.4799\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c214fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_rnn_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_rnn_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"my_rnn_model\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9538263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marks\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_save_path = \"my_rnn_model\"\n",
    "loaded_model = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa973c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_until_sep_token(starting_text, max_tokens=50):\n",
    "    text = starting_text\n",
    "    for _ in range(max_tokens):\n",
    "        # Tokenize the current text\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False) + [tokenizer.sep_token_id]\n",
    "        \n",
    "        # Ensure the sequence is within the model's acceptable input range\n",
    "        token_ids = token_ids[-(max_length-1):]\n",
    "        \n",
    "        # Predict the next token ID\n",
    "        input_sequence = tf.constant([token_ids], dtype=tf.int16)\n",
    "        predictions = loaded_model.predict(input_sequence)\n",
    "        predicted_token_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "        \n",
    "        # Break if the [SEP] token is predicted\n",
    "        if predicted_token_id == tokenizer.sep_token_id:\n",
    "            break\n",
    "        \n",
    "        # Append the predicted token to the text and continue\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        text += predicted_token\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd6f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Generated text: hello, how are you?iitiitbutbutbutandandandandandandandandandandandandandandandandandandandandandandandandandandandandandiiiiiiiiiiiiii\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"hello, how are you?\"\n",
    "generated_text = generate_until_sep_token(starting_text, max_tokens=50)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753992e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
